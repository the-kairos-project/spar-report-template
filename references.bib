% Example bibliography entries for SPAR reports
% Includes arXiv preprints, published papers, and web resources

@article{example_arxiv,
  author    = {Smith, John and Doe, Jane},
  title     = {An Example arXiv Preprint on Machine Learning Safety},
  journal   = {arXiv preprint arXiv:2301.12345},
  year      = {2023},
  eprint    = {2301.12345},
  archivePrefix = {arXiv},
  primaryClass = {cs.LG}
}

@article{example_published,
  author    = {Johnson, Alice and Williams, Bob},
  title     = {Foundations of AI Alignment Research},
  journal   = {Journal of Artificial Intelligence Research},
  volume    = {72},
  pages     = {1--45},
  year      = {2024},
  doi       = {10.1613/jair.1.12345}
}

@inproceedings{example_conference,
  author    = {Chen, Wei and Kumar, Raj},
  title     = {Scalable Methods for Neural Network Interpretability},
  booktitle = {Proceedings of the 38th International Conference on Machine Learning},
  pages     = {1234--1243},
  year      = {2023},
  publisher = {PMLR}
}

@misc{example_web,
  author    = {OpenAI},
  title     = {GPT-4 Technical Report},
  year      = {2023},
  url       = {https://openai.com/research/gpt-4},
  note      = {Accessed: 2024-01-15}
}

@book{example_book,
  author    = {Russell, Stuart and Norvig, Peter},
  title     = {Artificial Intelligence: A Modern Approach},
  edition   = {4th},
  publisher = {Pearson},
  year      = {2020},
  isbn      = {978-0134610993}
}

% ============================================================================
% References for example.tex
% ============================================================================

@article{brown2020language,
  author    = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and others},
  title     = {Language Models are Few-Shot Learners},
  journal   = {arXiv preprint arXiv:2005.14165},
  year      = {2020},
  eprint    = {2005.14165},
  archivePrefix = {arXiv},
  primaryClass = {cs.CL}
}

@article{wei2022chain,
  author    = {Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Ichter, Brian and Xia, Fei and Chi, Ed and Le, Quoc and Zhou, Denny},
  title     = {Chain-of-Thought Prompting Elicits Reasoning in Large Language Models},
  journal   = {arXiv preprint arXiv:2201.11903},
  year      = {2022},
  eprint    = {2201.11903},
  archivePrefix = {arXiv},
  primaryClass = {cs.CL}
}

@article{park2023ai,
  author    = {Park, Peter S. and Goldstein, Simon and O'Gara, Aidan and Chen, Michael and Hendrycks, Dan},
  title     = {AI Deception: A Survey of Examples, Risks, and Potential Solutions},
  journal   = {arXiv preprint arXiv:2308.14752},
  year      = {2023},
  eprint    = {2308.14752},
  archivePrefix = {arXiv},
  primaryClass = {cs.AI}
}

@article{hubinger2019risks,
  author    = {Hubinger, Evan and van Merwijk, Chris and Mikulik, Vladimir and Skalse, Joar and Garrabrant, Scott},
  title     = {Risks from Learned Optimization in Advanced Machine Learning Systems},
  journal   = {arXiv preprint arXiv:1906.01820},
  year      = {2019},
  eprint    = {1906.01820},
  archivePrefix = {arXiv},
  primaryClass = {cs.AI}
}

@misc{cotra2022scheming,
  author    = {Cotra, Ajeya},
  title     = {Without specific countermeasures, the easiest path to transformative AI likely leads to AI takeover},
  year      = {2022},
  url       = {https://www.lesswrong.com/posts/pRkFkzwKZ2zfa3R6H/without-specific-countermeasures-the-easiest-path-to},
  note      = {Accessed: 2024-01-15}
}

@article{elhage2021mathematical,
  author    = {Elhage, Nelson and Nanda, Neel and Olsson, Catherine and Henighan, Tom and Joseph, Nicholas and Mann, Ben and others},
  title     = {A Mathematical Framework for Transformer Circuits},
  journal   = {Transformer Circuits Thread},
  year      = {2021},
  url       = {https://transformer-circuits.pub/2021/framework/index.html}
}

@article{olah2020zoom,
  author    = {Olah, Chris and Cammarata, Nick and Schubert, Ludwig and Goh, Gabriel and Petrov, Michael and Carter, Shan},
  title     = {Zoom In: An Introduction to Circuits},
  journal   = {Distill},
  year      = {2020},
  doi       = {10.23915/distill.00024.001},
  url       = {https://distill.pub/2020/circuits/zoom-in/}
}

@article{cunningham2023sparse,
  author    = {Cunningham, Hoagy and Ewart, Aidan and Riggs, Logan and Huben, Robert and Sharkey, Lee},
  title     = {Sparse Autoencoders Find Highly Interpretable Features in Language Models},
  journal   = {arXiv preprint arXiv:2309.08600},
  year      = {2023},
  eprint    = {2309.08600},
  archivePrefix = {arXiv},
  primaryClass = {cs.LG}
}

@inproceedings{meng2022locating,
  author    = {Meng, Kevin and Bau, David and Andonian, Alex and Belinkov, Yonatan},
  title     = {Locating and Editing Factual Associations in GPT},
  booktitle = {Advances in Neural Information Processing Systems},
  volume    = {35},
  pages     = {17359--17372},
  year      = {2022}
}

@article{tenney2019bert,
  author    = {Tenney, Ian and Das, Dipanjan and Pavlick, Ellie},
  title     = {BERT Rediscovers the Classical NLP Pipeline},
  journal   = {arXiv preprint arXiv:1905.05950},
  year      = {2019},
  eprint    = {1905.05950},
  archivePrefix = {arXiv},
  primaryClass = {cs.CL}
}

@article{irving2018ai,
  author    = {Irving, Geoffrey and Christiano, Paul and Amodei, Dario},
  title     = {AI Safety via Debate},
  journal   = {arXiv preprint arXiv:1805.00899},
  year      = {2018},
  eprint    = {1805.00899},
  archivePrefix = {arXiv},
  primaryClass = {cs.AI}
}

% ============================================================================
% Add your own references here following BibTeX format
% Common entry types: article, inproceedings, book, misc, techreport
% ============================================================================
